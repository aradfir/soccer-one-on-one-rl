\chapter{مقایسه، آزمایش‌ها، و نتایج}
در این فصل به بررسی نتایج حاصل از پیاده‌سازی الگوریتم‌ها و مقایسه آن‌ها می‌پردازیم.
\section{فرآیند آزمایش}
برای آزمایش، ابتدا الگوریتم‌های معرفی‌ شده در فصل‌های قبل را با یک میلیون گام علیه دروازه‌بان کد پایه \lr{Agent}
آموزش می‌دهیم، و نمودار‌های فرآیند یادگیری را بررسی می‌کنیم. در هر ۵۰۰۰ گام از آموزش مدل به صورت تناوبی ذخیره می‌شود، و در نهایت بهترین مدل‌ها را انتخاب می‌کنیم.
لازم به ذکر است که منظور از گام، گام‌های یادگیری تقویتی است، که هرکدام متناظر با یک ضربه به توپ اند و منظور گام‌های شبیه‌ساز فوتبال نیست.

در نهایت مدل انتخاب شده را برای ۱۰۰ ضربه پنالتی علیه کد پایه قرار می‌دهیم تا با سیاست حریصانه و بدون تصمیم‌گیری تصادفی سنجیده شود، و درصد وقوع هر یک از حالت‌های پایانی بازی را بررسی می‌کنیم.
در نهایت راهکار‌های متفاوتی را برای بهبود نتایج یادگیری امتحان می‌کنیم.

تمامی آزمایش‌ها روی یک سیستم با پردازنده‌ی \lr{AMD Ryzen 7 6800H}، 
حافظه ۳۲ گیگابایت،
و کارت گرافیک \lr{NVIDIA RTX 3070Ti} انجام شده است، 
همراه‌ با کتاب‌خانه‌های کودا 
\LTRfootnote{CUDA}
\lr{CUDA 12.4}, \lr{cudnn 8.9.2}، و \lr{cublas 12.1.3}
انجام شده‌است.
\section{ارزیابی الگوریتم‌ها}
در ابتدا الگوریتم \lr{DQN} را برای یادگیری از صفر امتحان می‌کنیم.
بعد از یک میلیون گام آموزش که حدود ۱۲ ساعت طول کشید، عامل موفق به کشف تکنیک گل زدن نشد.
فرآیند آموزش ۵ بار و با ابرپارامتر‌های متفاوت آزمایش شد، اما در هیچ یک از اجرا‌ها عامل موفق به یادگیری نشد.
یکی از دلایل ممکن این موضوع، می‌توانست تفاوت زیاد بین پاداش‌های موفق و ناموفق باشد، چرا که این موضوع ابعاد گرادیان را بزرگ کرده و باعث عدم ثبات می‌شود. به همین منظور یک ضریب مقیاس‌گر 
برای پاداش‌ها اضافه شد، که باز هم موثر در موفقیت نبود.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/DQN_graphs.png}
    \caption{نمودار سه پارامتر متفاوت برای یک اجرای الگوریتم \lr{DQN}}\label{fig:dqn_graphs}
\end{figure}

در شکل \ref{fig:dqn_graphs}
می‌توان به ترتیب میانگین پاداش عامل، نرخ کاوش، و خطای شبکه عصبی را در طی زمان آموزش دید.
محور افقی این نمودار گام‌های آموزش است، که در اینجا ۱ میلیون گام است.
مقیاس پاداش‌ها برای این اجرا، ۰.۲
بوده‌است و برای رسم نمودار، به مقیاس ۱ تبدیل شده‌است.
همانطور که می‌توان مشاهده کرد، نمودار خطای شبکه از گام ۵۰۰۰۰ آغاز شده. این به این علت است که آموزش شبکه این گام آغاز می‌شود، تا پیش از آن بافر با حالت‌های متفاوت پر شود.

همانطور که قابل مشاهده است، پاداش‌ها نسبت به حالت آغازین افزایش می‌یابند اما عامل موفق به کشف حالت گل زدن نشده‌است.
میزان خطای شبکه عصبی نیز در طول زمان کاهش یافته و نزدیک به ۱ شده، اما این هم نشان‌دهنده‌ی یادگیری موفق نیست، بلکه شبکه همواره (به درستی)
میزان ارزش حالت شکست را خروجی می‌دهد.

همین فرآیند را با الگوریتم \lr{DDPG} تکرار می‌کنیم.
می‌توان مشاهده کرد که عامل در زیر ۱۰۰ هزار گام موفق به کشف تکنیک گل زدن شده‌است.
همچنین همانطور که در فصل‌های قبلی ذکر شد، به علت محدود بودن اندازه بافر، عامل ممکن است یادگیری خود را فراموش کند.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/DDPG_graphs.png}
    \caption{نمودار سه پارامتر متفاوت برای یک اجرای الگوریتم \lr{DDPG}}\label{fig:ddpg_graphs}
\end{figure}

در شکل \ref{fig:ddpg_graphs}
می‌توان به ترتیب خطای شبکه بازیگر، خطای شبکه منتقد، و میانگین پاداش عامل را در طی زمان آموزش دید.
همانطور که می‌توان مشاهده کرد، در حین موفقیت عامل خطای شبکه‌ها و به ویژه خطای بازیگر کاهش یافته‌است.

\begin{table}[H]
    \centering
    \caption{بهترین نتایج الگوریتم‌ها مقابل کد پایه \lr{Agent2D} برای صد پنالتی.}
    \begin{tabular}{ |p{4cm}|p{2cm}|p{2cm}|  }
        \hline
        خروجی & \lr{DQN} & \lr{DDPG} \\
        \hline
        گل زدن & ۲ & ۹۹\\
        \hline
        بیرون رفتن توپ & ۱۸ & ۰\\
        \hline
        گرفتن توسط دروازه‌بان & ۷۴ & ۱\\
        \hline
        اتمام زمان & ۶ & ۰\\
        \hline
        \end{tabular}
        \label{tab:base_results}
\end{table}

با توجه به اینکه الگوریتم \lr{DDPG} موفق بود، می‌توان حدس‌هایی راجع به عدم موفقیت \lr{DQN} زد.
یکی از دلایل، این است که به علت گسسته بودن اعمال، تعداد گام‌های لازم برای رسیدن به گل بیشتر است، و از آنجا که عامل باید به صورت تصادفی گل زدن را کشف کند،
عامل به تعداد دفعات کافی به این حالت نمی‌رسد.
همچنین همانطور که در شکل \ref{fig:actor_critic} می‌توان دید، شبکه در الگوریتم
\lr{DQN} به ازای هر عمل، یک خروجی دارد و به همین علت، فضای عمل بزرگ یادگیری را به مراتب دشوار می‌کند.
از این رو می‌توان سه راه را برای بهبود یادگیری این الگوریتم پیشنهاد داد:
\begin{enumerate}
    \item بهبود گسسته‌سازی عمل‌ها.
    \item استفاده از پاداش های بهتر برای حالت‌های غیرنهایی که عامل را به سمت حالت‌های مفید‌تر هدایت کند.
    \item تغییر فضای عمل و جدا‌سازی اعمال به دریبل‌ زدن و شوت زدن.
\end{enumerate}

\section{بهبود تابع پاداش}
برای بهبود تابع پاداش، به روش گل‌زنی الگوریتم \lr{DDPG} نگاه می‌کنیم.
به نظر می‌آید که نزدیک شدن دروازه‌بان به توپ عامل منفیی نیست، چرا که این حالت ممکن است به عامل کمک کند تا با یک ضربه به توپ، زاویه دروازه را باز کند.
همچنین در صورت نزدیکی بیش از حد دروازه‌بان به توپ در حین ضربه، عامل می‌تواند از قابلیت گسسته بودن زمان شبیه‌ساز استفاده کند و با انجام قوی‌ترین ضربه ممکن، توپ را از روی دروازه‌بان بگذراند.
بنابرین برای گام‌های بعدی، عامل فاصله با دروازه‌بان را از محاسبات پاداش حذف می‌کنیم.


\section{جدا‌سازی عمل شوت}
عامل در ابتدای یادگیری، هنوز به درک اینکه در چه حالتی می‌تواند شوت منجر به گل داشته باشد نرسیده است.
از آنجا که این عمل با عمل رسیدن به موقعیت شوت‌زنی متفاوت است، عامل ممکن است دیر به دیر به حالت‌هایی که شوت زدن در آن ممکن است برسد.
بنابرین در حین یادگیری نسبت به ضربه با سرعت‌های بالا بدبین می‌شود.

برای حل این مشکل، فضای عمل را به گونه‌ای اصلاح می‌کنیم که شوت زدن جدا باشد.
\subsection{شوت با رفتار سطح بالای کد پایه}
یک عمل خاص مدنظر می‌گیریم که در صورت انتخاب شدن توسط عامل، شوت کد پایه \lr{Agent2D} 
انجام می‌شود. این کد در صورت وجود شوت ممکن، آن را انجام می‌دهد و در غیر این صورت، هیچ عملی انجام نمی‌شود.
در صورتی که عامل این عمل را انتخاب کند ولی به گل نرسد، پاداش منفیی به آن داده می‌شود، تا عامل استفاده صحیح از این ابزار را یاد بگیرد.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \pie[rotate=90, color={green, red, orange, yellow}]
        {
            3/گل,
            12/توپ رفتن بیرون,
            49/دروازه‌بان توسط گرفتن,
            36/زمان اتمام
        }
    \end{tikzpicture}
    \caption{نتیجه ۱۰۰ ضربه پنالتی با شوت سطح بالا مقابل کد پایه \lr{Agent2D} برای عامل یادگیری‌شده}\label{fig:helios_shoot_pie}
\end{figure}

طبق تجربه پیش از این پروژه، این رفتار سطح بالا بسیار محافظ‌کارانه تصمیم می‌گیرد.
از این رو شاید کمک چندانی به عامل برای یادگیری نکند.
همچنین از آنجا که استفاده از این قابلیت بسیار وابسته به شبیه‌سازی ضربه به توپ و بررسی امکان قطع توپ توسط دروازه‌بان است، خلاف ماهیت انجام این پروژه است.
\subsection{عمل شوت به نقاط ثابت دروازه}
فرض کنید تعدادی نقطه ثابت دروازه را انتخاب کنیم.
هر یک از این نقاط متناظر با یک عمل شوت است.
در این عمل، به توپ با حداکثر سرعت ممکن در راستای این عمل ضربه می‌زنیم.
برای انتخاب نقاط، دو تیرک دروازه را به عنوان نقاط ثابت در نظر گرفته، و فضای بین این نقطه را به $n-1$ نقطه تقسیم می‌کنیم.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \pie[rotate=90, color={green, red, orange, yellow}]
        {
            19/گل,
            4/توپ رفتن بیرون,
            53/دروازه‌بان توسط گرفتن,
            24/زمان اتمام
        }
    \end{tikzpicture}
    \caption{نتیجه تست ۱۰۰ پنالتی مقابل کد پایه \lr{Agent2D} با ۵ نقطه شوت.}\label{fig:custom_shoot_pie}
\end{figure}

\section{تغییر گسسته‌سازی}
حال که عامل دارای عمل مستقیم شوت است، می‌توانیم گسسته‌سازی قدرت را به گونه‌ای تغییر دهیم که سرعت‌های بالا را نداشته باشد.
برای این منظور ضربه به توپ را به سه سرعت تقسیم می‌کنیم: ۱۵ درصد حداکثر سرعت توپ، ۲۵ درصد حداکثر سرعت توپ، و ۵۰ درصد حداکثر سرعت توپ.
در واقع اعمال ممکن را به شوت به نقطه یا دریبل با سرعت و زاویه معین تقسیم می‌کنیم.

همچنین به صورت شهودی می‌توان برداشت کرد که تقسیم زوایای جلوی بازیکن به شدت مهم‌تر از زوایای پشت بازیکن است، 
چرا که به ندرت نیاز می‌شود بازیکن با ضربه به عقب پیشرفت کند.
زوایای جلوی بازیکن را به ۹ قسمت تقسیم می‌کنیم، و دو راستا برای دریبل به عقب (بالا عقب و پایین عقب) اضافه می‌کنیم.

با ترکیب این دو تکنیک، فضای حالت از ۱۲۰ عمل، به ۳۸ عمل کاهش می‌یابد.
این کاهش از دو راستا به یادگیری ما کمک می‌کند:
\begin{itemize}
    \item کاهش ابعاد گرادیان و افزایش سرعت یادگیری
    \item افزایش احتمال انجام عمل مفید به کمک کاوش، به علت حذف رفتار‌هایی که به احتمال زیاد مفید نیستند
\end{itemize}
پس از یک میلیون گام آموزش، نمودار پاداش‌ها و درصد موفقیت در حین آموزش را مشاهده می‌کنیم.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/dqn_discretization.png}
    \caption{نمودار میانگین نرخ گل‌زنی و میانگین پاداش \lr{DQN} با تغییرات گسسته‌سازی}\label{fig:discretization_change}
\end{figure}
همانطور که در تصویر مشاهده می‌کنید، با این تغییرات عامل موفق به کشف روش گل‌زنی و موفقیت ۷۲ درصد شده‌است.
همچنین می‌توان دید که نمودار پاداش و نرخ گل‌زنی هم‌رفتار و تقریبا هم‌شکل هستند.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \pie[rotate=90, color={green, red, orange, yellow}]
        {
            74/گل,
            2/توپ رفتن بیرون,
            24/دروازه‌بان توسط گرفتن,
            0/زمان اتمام
        }
    \end{tikzpicture}
    \caption{نتیجه تست ۱۰۰ پنالتی مقابل کد پایه \lr{Agent2D} با ۵ نقطه شوت.}\label{fig:discretization_pie}
\end{figure}

\section{بررسی تعمیم پذیری}
در این بخش، با تست علیه دروازه‌بان تیم‌هایی که مقابل آن‌ها یادگیری رخ نداده، می‌بینیم که آیا این یادگیری تعمیم‌پذیر بوده یا خیر.
با توجه به اینکه پس از مسابقات، فایل‌های اجرایی تیم‌ها در اختیار سایر شرکت‌کنندگان قرار می‌گیرد، می‌توانیم این تست را انجام دهیم.
از ۳ تیم برای این منظور استفاده می‌کنیم:
تیم 
\lr{Helios2023}
 که قهرمان پیاپی چندین دوره است، تیم کد پایه \lr{Agent2D}
 که مبنای بسیاری از سایر تیم‌ها است،
تیم \lr{YuShan2023} که از تیم‌های موفق دیگر است و استراتژی متفاوتی نسبت به دو تیم قبلی دارد.

\begin{table}[H]
    \centering
    \caption{نتایج تست الگوریتم \lr{DDPG} علیه تیم‌های مختلف}\label{tab:generalization}
        \begin{tabular}{ |p{4cm}|p{2cm}|p{2cm}|p{2cm}|  }
            \hline
            خروجی & \lr{Agent2D} & \lr{Helios2023} & \lr{YuShan2023}\\
            \hline
            گل زدن & ۹۹ & ۴۰ & ۲۷ \\
            \hline
            بیرون رفتن توپ & ۰ &۴ & ۱۱ \\
            \hline
            گرفتن توسط دروازه‌بان & ۱ &۴۰ & ۵۷ \\
            \hline
            اتمام زمان & ۰ &۱۶ & ۴ \\
            \hline
        \end{tabular}
\end{table}
به صورتی شهودی و دیدن نتایج تست، می‌توان دید که نتایج تا حدودی قابل تعمیم است، اما شوت زدن عامل بسیار وابسته به مختصات دروازه‌بان است و از این رو، شوت‌های به بیرون و یا گرفته شده توسط دروازه‌بان افزایش یافته‌اند.
همچنین دروازه‌بان سایر تیم‌ها مانند \lr{Helios2023}،
 در شرایطی که نتوانند به توپ برسند از دستور تکل استفاده می‌کنند، که برد بیشتری نسبت به گرفتن توپ دارد ولی به صورت احتمالاتی است. در صورت تکل موفق حالت گرفتن توسط دروازه‌بان ثبت می‌شود.

\begin{table}[H]
    \centering
    \caption{نتایج تست الگوریتم \lr{DQN} بهبود یافته مقابل تیم‌های مختلف}\label{tab:dqn_generalization}
        \begin{tabular}{ |p{4cm}|p{2cm}|p{2cm}|p{2cm}|  }
            \hline
            خروجی & \lr{Agent2D} & \lr{Helios2023} & \lr{YuShan2023}\\
            \hline
            گل زدن & ۷۱ & ۰ & ۱۰ \\
            \hline
            بیرون رفتن توپ & ۰ &۰ & ۱۵ \\
            \hline
            گرفتن توسط دروازه‌بان & ۲۹ &۱۰۰ & ۷۰ \\
            \hline
            اتمام زمان & ۰ &۰ & ۵ \\
            \hline
        \end{tabular}
\end{table}
با توجه به گسسته‌سازی، هر گونه تغییر در موقعیت دروازه‌بان عمل بعدی عامل را به شدت تحت تاثیر قرار می‌دهد و عامل به شدت با حالت‌های از پیش دیده فاصله می‌گیرد.
از این رو همانطور که پیش‌بینی می‌شد، تعمیم‌پذیری این الگوریتم به تیم‌های دیگر بسیار پایین است.

در نهایت یادگیری الگوریتم \lr{DDPG} را تکرار می‌کنیم، با این تفاوت که آموزش را از ابتدا و مقابل تیم‌های مختلف انجام می‌دهیم.
این آزمایش از این رو است که ببینیم در صورتی که آموزش روی تیم‌های قوی‌تر انجام شود، آیا عامل موفق به یادگیری تکنیک‌های مفید‌تری می‌شود یا خیر.

فرآیند آموزش تا زمانی ادامه می‌یابد که عامل به تکنیک گل‌زنی مقابل تیم آموزشی برسد، و تا یک میلیون گام ادامه نخواهد یافت.
برای یادگیری مقابل تیم \lr{Helios2023} فقط ۱۵۰ هزار گام کافی بود تا نرخ موفقیت به بالا ۹۰ درصد برسد که حدود یک ساعت زمان برد. یادگیری مقابل تیم \lr{YuShan2023}  حدود ۷۰ هزار گام طول کشید که ۲۰ دقیقه زمان برد.


\begin{table}[H]
    \centering
    \caption{نتایج تست الگوریتم \lr{DDPG} علیه تیم‌های مختلف با آموزش مقابل تیم \lr{Helios2023}}\label{tab:ddpg_helios_generalization}
        \begin{tabular}{ |p{4cm}|p{2cm}|p{2cm}|p{2cm}|  }
            \hline
            خروجی & \lr{Agent2D} & \lr{Helios2023} & \lr{YuShan2023}\\
            \hline
            گل زدن & ۴۷ & ۹۰ & ۲۳ \\
            \hline
            بیرون رفتن توپ & ۱۶ &۰ & ۵ \\
            \hline
            گرفتن توسط دروازه‌بان & ۳۷ &۷ & ۶۵ \\
            \hline
            اتمام زمان & ۰ &۳ & ۷ \\
            \hline
        \end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \caption{نتایج تست الگوریتم \lr{DDPG} علیه تیم‌های مختلف با آموزش مقابل تیم \lr{YuShan2023}}\label{tab:ddpg_yushan_generalization}
        \begin{tabular}{ |p{4cm}|p{2cm}|p{2cm}|p{2cm}|  }
            \hline
            خروجی & \lr{Agent2D} & \lr{Helios2023} & \lr{YuShan2023}\\
            \hline
            گل زدن & ۰ & ۰ & ۹۹ \\
            \hline
            بیرون رفتن توپ & ۰ &۰ & ۰ \\
            \hline
            گرفتن توسط دروازه‌بان & ۸۳ &۱۰۰ & ۱ \\
            \hline
            اتمام زمان & ۱۷ &۰ & ۰ \\
            \hline
        \end{tabular}
\end{table}

به نظر می‌رسد که استراتژی متفاوت \lr{YuShan2023} باعث می‌شود عامل‌هایی که روی سایر تیم‌ها آموزش دیده‌اند گیج شوند و نتوانند به موفقیت برسند.
اما آموزش روی \lr{YuShan2023} که به دور زدن دروازه‌بان در آن آسان‌تر است، باعث می‌شود که عامل 
به موفقیت جلوی سایر تیم‌ها نرسد، چرا که تاکتیک موفق جلوی \lr{YuShan2023} منجر به گرفتن توسط دروازه‌بان می‌شود.

از سوی دیگر، با توجه به این که استراتژی \lr{Helios2023} مشابه ولی قوی‌تر نسبت به \lr{Agent2D} است، آموزش مقابل آن باعث می‌شود که عامل به موفقیت نسبی نزدیک شود.

لازم به ذکر است که حتی اگر عامل در دو دور آموزش مقابل تیم آموزشی به نتایج یکسان برسد، نمی‌توان انتظار داشت که نتایج تست مقابل سایر تیم‌ها یکسان باشد. به این منظور که اگر آموزش را دو بار از صفر آغاز کنیم و تا صد درصد موفقیت پیش ببریم، ممکن است نتایج تست مقابل تیم‌های دیگر بین دو حالت نزدیک نباشد. از این رو این آزمایش معیار خیلی دقیقی برای بررسی کمی تعمیم‌پذیری نیست.

در آینده می‌توان راه‌های تولید مدل‌های تعمیم‌پذیر‌تر را بیشتر بررسی کرد. یکی از این راه‌ها استفاده از حریف‌های متفاوت در حین یادگیری، و روش‌های یادگیری تقویتی مداوم\LTRfootnote{Continual Reinforcement Learning} است.

\section{جمع‌بندی}
در این فصل، ما الگوریتم‌های یادگیری تقویتی \lr{DQN} و \lr{DDPG} را برای یادگیری تکنیک گل‌زنی در فوتبال تک به تک مورد بررسی قرار دادیم.
همچنین چند راه حل برای بهبود یادگیری این الگوریتم‌ها ارائه دادیم.
در نهایت، تعمیم‌پذیری این الگوریتم‌ها را بررسی کردیم و دیدیم که طبق توقع، عامل در مقابل حریف‌هایی که از پیش ندیده، قدرت یادگیری کمتری دارد.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% در بخش پایانی گزارش، جمعبندي و مروري بر سیر مطالب عنوان شده در گزارش خواهیم داشت. همچنین نتایج حاصل را بیان
% کرده و پیشنهادهایی براي ادامه کار در این موضوع ارائه میدهیم.
% \section{نتیجه‌گیری}
% این گزارش ۶ الگوریتم‌ برای پیش‌بینی مصرف انرژی ساختمان‌ها را مورد بررسی قرار داد که برای این الگوریتم‌ها داده‌های مصرف آینده‌ی ساختمان‌ها و مصرف گذشته‌شان موجود بود. 
% همچنین در این مقاله اهمیت برخی متغیرها مانند وضعیت دمای محیط و تعداد ساکنین ساختمان در هر زمان نیز مورد توجه قرار گرفتند و اهمیت آن‌ها در تصمیم گیری مشخص گردید.
% چندین مدل یادگیری ماشین موفق با استفاده از داده‌های انرژی ثبت‌شده گذشته برای پیش‌بینی کوتاه‌مدت، میان‌مدت و بلندمدت توسعه یافته‌اند. مشاهده می شود 
% که هر یک از تکنیک های توصیف شده دارای مجموعه ای از مزایا و معایب است. اینها با توجه به تجزیه و تحلیل داده های انرژی ساختمان به تفصیل تجزیه و تحلیل و ارائه شده اند.
%  تأکید ویژه بر مدل ترکیبی داده شده است، که ترکیبی از دو یا چند تکنیک یادگیری ماشینی است به نحوی که هر مدل قدرت دیگری 
%  را تحسین می کند. به عنوان مثال، یک مدل ترکیبی که میانگین متحرک خودهمبسته یکپارچه و الگوریتم‌های تکاملی را در نظر می‌گیرد، می‌تواند
%   از مدل میانگین متحرک خودهمبسته یکپارچه برای تعیین تناوب و خطی بودن استفاده کند، در حالی که الگوریتم‌ تکاملی می‌تواند به طور موثر باقیمانده‌ها را تعیین کند. ترکیبات مختلفی از مدل ترکیبی و تازگی آنها در
%    ادبیات شناسایی شده و به طور سیستماتیک در این مقاله ارائه شده است. مشاهده می‌شود
%    که ترکیب تکنیک‌های پیش‌بینی سری‌های زمانی مانند شبکه ی عصبی مصنوعی، میانگین متحرک خودهمبسته یکپارچه به خوبی با 
%    تکنیک‌های بهینه‌سازی ترکیب می‌شوند. چنین ترکیب‌هایی به طور گسترده در تحقیقات اختصاص یافته به بهینه‌سازی ساختمان مورد بررسی قرار گرفته‌اند.
%     انتظار می رود روند رو به رشد
%     در تحقیقات در بهره وری انرژی ساختمان در پرتو انگیزه پایداری جهانی ادامه یابد. این امر نظارت و پیش بینی داده های انرژی در 
%     زمان واقعی را در این زمینه مرتبط و حیاتی می کند. این مقاله خلاصه‌ای جامع از تکنیک‌های پیش‌بینی موجود همراه با ترکیبی
%      از مدل ترکیبی ارائه می‌کند و راه را برای تحقیقات آینده در زمینه مصرف انرژی ساختمان هموار می‌کند.
%      \\
%      حوزه بهینه سازی ساختمان بر اساس یک شبکه گسترده جمع آوری داده، نظارت، پیش بینی، بهینه سازی و کنترل است. تمام این زیرساخت‌ها
%       همواره به کل هزینه عملیاتی ساختمان می‌افزایند. چالش در اینجا بررسی مزایای اضافه شده از نظر هزینه سرمایه گذاری و هزینه به دست آمده
%       به دلیل صرفه جویی در انرژی ناشی از بهینه سازی ساختمان است. مطالعات کمی وجود دارد که بخش مالی را برای کنترل عملکرد و بهینه سازی ساختمان 
%      برجسته می کند. این محدودیت در به اشتراک گذاری داده های هزینه یا به دلیل 
%      نیروهای بازار درگیر است یا به دلیل محرمانه بودن ماهیت داده های درگیر. لبی الدان\LTRfootnote{Labeodan} 
%      و همکاران. کاربردهای شبکه حسگرها و محرک‌های بی‌سیم کم‌هزینه را برای مدل‌سازی اشغال و کنترل روشنایی در یک ساختمان
%       اداری مورد بحث قرار داد \cite{labeodan2016application}. هزینه کل سیستم تقریبا 2575 یورو برای 12 ایستگاه کاری بود که
%       شامل حسگرهای حرکتی بی سیم و سنسورهای صندلی بود. نتایج نشان می دهد که به طور متوسط 24\% کاهش در مصرف انرژی روشنایی برای یک دوره
%       دو هفته ای با هزینه اجرا شده در حدود 215 یورو برای هر ایستگاه کاری است. نویسندگان خاطرنشان کردند که هزینه اولیه بالاتر و عدم آگاهی از عوامل مؤثر
%       در کاهش سرعت استقرار حسگرها هستند. با این حال، صرفه جویی در انرژی به دست آمده، سهولت استقرار و بهبود سنجش محیطی این
%       را به عنوان یک راه حل مناسب برای دستیابی به عملکرد بهبود 
%       یافته ساختمان نشان می دهد. کومار و همکاران همچنین متوجه شد که هزینه سنسورهای نظارت کنترل کیفیت هوای داخلی الزامات استقرار در مقیاس بزرگ برای کنترل و اتوماسیون را
%        برآورده نمی کند \cite{kumar2016real}. لیلیس و همکاران اشاره کنید
%        که علاقه به راه‌حل‌های 
%       مبتنی بر اینترنت اشیا\LTRfootnote{Internet Of Things (IOT)} مدرن برای بهینه‌سازی ساختمان به دلیل عدم برآورد منافع هزینه، مهار شده است \cite{lilis2017towards}. چن و همکاران اشاره کرد
%        که هزینه مربوط به مصرف انرژی مجموعه تصادفی ساختمان‌ها در چین با سیستم های اتوماسیون ساختمان تقریبا دو برابر ساختمان های
%        بدون سیستم اتوماسیون ساختمان است \cite{chen2016cost}. این به دلیل نقص سنسور و نقص استراتژی کنترل است که منجر به افزایش قابل توجهی در مصرف انرژی نهایی می شود.
%         \\
%         چند مطالعه مروری شبکه حسگر بی سیم را پوشش داده است که سیستم مدیریت انرژی ساختمان را برای کاربردهای خانه/ساختمان 
%         هوشمند فعال کرده است \cite{kazmi2014review,kuzlu2015review}. از آنجایی که فناوری کنترل پیش‌بینی مدل هنوز در مرحله توسعه است و 
%         نیاز به بهینه‌سازی سنگین بر اساس نوع و عملکرد ساختمان دارد،
%          پیاده‌سازی در حال حاضر بیشتر بر روی بستر آزمایشی و اعتبارسنجی تمرکز دارد. در عین حال، این فناوری هنوز
%          با هزینه لازم برای استقرار در مقیاس های بزرگ در دسترس نیست. 
%          این چالش ها منجر به نفوذ آهسته اتوماسیون ساختمان و بهینه سازی در کل می شود. دامنه این مقاله مروری، با این حال
%         ، محدود به مطالعه تکنیک‌های پیش‌بینی سری‌های زمانی برای مصرف انرژی ساختمان است که بخشی جدایی‌ناپذیر
%          از فرآیند بهینه‌سازی و کنترل ساختمان است.
% \section{پیشنهادها}
%     به طور کلی الگوریتم‌های هوش‌ مصنوعی که در زمینه‌ی پیش‌بینی سری داده‌های زمانی کار میکنند و بخش عمده‌ی آن‌ها که الگوریتم‌های یادگیری ماشین می‌شوند مشکلات و سختی‌های مخصوصی دارند
%     از جمله محدودیت‌های مربوط به یادگیری‌آن‌ها که نیازمند داده‌های بسیتر زیاد برای یادگیری و آموزش میباشد و هم‌چنین نیاز به توان پردازشی بالای آن‌ها که از جمله مشکلات روش‌های هوش مصنوعی 
%     به طور کلی میباشد. 
%     برای حل این مشکل پیشنهاد میشود که الگوریتم‌های نوینی با استفاده از روش‌های ترکیبی توسعه داده بشوند که نیاز به داده‌های زیاد برای یادگیری در آن‌ها کمتر باشد و بتوانند با شبیه‌سازی 
%     آموزش ببینند و همچنین برای حل مشکل پردازش‌های سنگین با تحقیقات جدید به سمت برخط کردن یادگیری الگوریتم‌های هوش مصنوعی حرکت بکنیم به این صورت که تمام پردازش‌های مورد نیاز 
%     الگوریتممان برروی سرورهای قدرتمندی در سطح منطقه انجام بشوند و ساختمان‌ها مشکل پردازشی‌شان از این نظر مرتفع بشود و تنها برای انجام اندازه‌ی مشخصی از پردازش برروی شبکه ی قدرتمندمان 
%     هزینه پرداخت کنند تا هزینه‌هایشان کاهش پیدا بکند.